#####################################
# Stanford University CS231n Course #
#####################################


# Lecture 1 | Introduction to Convolutional Neural Networks for Visual Recognition
 = https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv

# (incomplete) Lecture 2 | Image Classification 
 = https://www.youtube.com/watch?v=OoUX-nOEjG0&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=2

# Lecture 4 | Introduction to Neural Networks
 = https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4
	- (6:00) Back Propagation
	- (1:04:22) Full Implementation of training a 2-layer Neural Network (Code ~20 lines)
	- (1:10:25) Activation Functions

# Lecture 5 | Convolutional Neural Networks
 = https://www.youtube.com/watch?v=bNb2fEVKeEo&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=5
 	- (14:35) CNN
 		- (N-F)/stride + 1 
 	- (45:25) Summary of Conv Layer
 	- (47:40) Conv Layer in Torch
 	- (47:50) Conv Layer in Caffe
 	- (54:25) Pooling Layer
 	- Link to Andrej Karpathy's CIFAR-10 demo: https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html

# Lecture 6 | Training Neural Networks I
 = https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=6
 	- (5:00) Activation Functions
 		- (5:05) Sigmoid 
 		- (13:05) tanh
 		- (13:37) ReLU 
 		- (21:50) Leaky ReLU
 		- (22:30) Parametric Rectifier - PReLU
 		- (23:00) Exponential Linear Unit - ELU
 		- (25:15) Maxout "Neuron"
 	- (27:40) Step 1: Preprocessing Data 
 	- (34:30) Weight Initialization
 		- (44:45) Xavier Initialization
 		- (48:40) Literature in Proper Initialization
 	- (48:55) Batch Normalization

# Lecture 7 | Training Neural Networks II
 = https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=7
 	- (23:25) SGD + Momentum (Adding velocity to Gradient Descent to counter Saddle points) (SGD = Stochastic Gradient Descent)
 	- (27:45) Nesterove Momentum
 	- (33:17) AdaGrad 
 		- Good for Convex, bad for non-convex
 	- (35:45) RMSProp (modification to AdaGrad)
 	- (38:55) Adam
 	- (55:25) Regularization
 		- (55:45) Regularization: Dropout
 	- (1:04:45) Data Augmentation
 	- (1:09:40) Transfer Learning

# Lecture 8 | Deep Learning Software
 = https://www.youtube.com/watch?v=6SlgtELqOWc&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=8
 	- (3:45) CPU vs GPU
 	- (5:35) NVIDIA vs AMD
 	- (12:00) Programming GPUs
 		- CUDA - C-like code runs directly on GPUs
 		- OpenCL - similar to CUDA; run on anything
 	- (24:00) Computational Graphs
 		- Numpy only runs in CPU. To solve this PyTorch and TensorFlow both allow to calculate operations and Gradient on GPUs.
 	- (25:38) TensorFlow code to calculate Gradient
 	- (26:35) PyTorch to calculate Gradient
 	- (27:40) TensorFlow: Neural Net
 	- (47:45) Kerras: High-Level Wrapper 
 	- (49:15) TensorFlow: Other High-Level Wrappers
 	- (51:20) PyTorch
 	- (1:02:25) Static (TensorFlow) vs Dynamic (PyTorch) Computational Graph
 	- (1:12:40) Caffe

# Lecture 10 | Recurrent Neural Networks
 = https://www.youtube.com/watch?v=6niqTuYFZLQ&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=10
 	- (9:30) Recurrent Neural Networks: Process Sequence
 	- (13:36) Recurrent Neural Network
 	- (42:45) Image Captioning
 	- (44:35) Image Captioning with Attention
 	- (47:55) Visual Question Answering 
 	- (51:25) Vanilla RNN Gradient Flow
 	- (55:45) Long Short Term Memory (LSTM)


# Lecture 13 | Generative Models
 = https://www.youtube.com/watch?v=5WoItGTWV54
 	- (1:15) Supervised vs Unsupervised Learning
 		- (1:15) Supervised Learning
 		- (2:05) Unsupervised Learning
 			- Clustering (KMeans)
 			- Dimensionality Reduction (PCA)
 			- Feature Learning (Autoencoders)
 			- Density Estimation (1D -> 2D)
 	- (5:35) Generative Models
 	- (9:15) Fully Visible Belief Network
 	- (11:05) PixelRNN
 	- (12:25) PixelCNN
 	- (17:35) Generation Samples
 	- (18:15) PixelRNN and PixelCNN (Pros and Cons)
 	- (19:35) Variational Autoencoders (VAE)
 	- (20:50) Autoencoders: Background
 		- (22:20) Why dimension reduction? 
 			= Because we want the most important and meaningful features (z) among all the features (x)
 			= We want to get the lowest # of z features that can successfully reconstruct input data X
 			= Use L2 Loss function to see how similar are the reconstructed images
 	- (27:20) Variational Autoencoders (VAE)
 		- (30:50) What is the problem with maximizing probability in VAE?
 			= The intigral part is not tractible (i.e. it is not easy to solve)
 	- (31:00) Variational Autoencoders: Intractibility
 	- (35:25) VAE Log data likelihood calculation 
 	- (44:00) VAE Generating Data (Examples: Numbers, Faces)
 	- (47:15) VAE Pros and Cons
 	- (50:00) GANs Generative Adversarial Networks 
 	- (52:40) Training GANs: Two Player Game
 		- (59:00) Gradient ascent and Gradient descent
 	- (1:03:00) Full GANs Algorithm 
 	- (1:06:20) GANs Generated Samples
 		- Numbers, Faces, CIFAR-10
 	- (1:07:20) GANs: Convolutional Architectures
 		- (1:08:00) The samples are better using GANs+CNN
 	- (1:09:00) GANs: Interpretable Vector Math
 	- (1:10:20) 2017: Year of the GAN
 	- (1:13:15) The GAN Zoo
 	- (1:14:00) GANs: Pros and Cons
 	- 


 	- Extra Materials: 
	# Variational Autoencoders (VAE) Intuition and Code (Keras):
 		= https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/

# Lecture 14 | Deep Reinforcement Learning
 = https://www.youtube.com/watch?v=lvoHnicueoE&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=14
 	- (6:25) Markov Decision Process

# Lecture 15 | Efficient Methods and Hardware for Deep Learning
 = https://www.youtube.com/watch?v=eZdOkDtYMoo&index=15&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv
 	<Watched until (41.27)>
