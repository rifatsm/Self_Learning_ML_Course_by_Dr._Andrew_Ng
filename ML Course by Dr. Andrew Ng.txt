##################################
# ML Course by Dr. Andrew Ng.txt #
##################################

###########################
# Support Vector Machines #
###########################

# Lecture 12.1 — Support Vector Machines | Optimization Objective — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=hCOIMkcsm_g&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=70
 	- (3:00) Concept of Cost Function of SVM (in graph)
 	- (7:52) Logistic Regression vs SVM (Deriving SVM Cost Function equation)
 	- (13:52) SVM Hypothesis

# Lecture 12.2 — Support Vector Machines | Large Margin Intuition — [Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=Ccje1EzrXBU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=71
 	- (0:15) Concept of Cost Function of SVM (in graph) (again)
 	- (4:42) SVM Decision Boundary: Linearly separable case, Large margin classifier
 	- (7:12) Presence of Outliers
 
# Lecture 12.3 — Support Vector Machines | Mathematics Behind Large Margin Classification (Optional)
 = https://www.youtube.com/watch?v=QKc3Tr7U4Xc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=72

# Lecture 12.4 — Support Vector Machines | (Kernels-I) — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=mTyT-oHoivA&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=73
 	- (2:15) Kernel/Gaussian Kernal/Similarity Kernel
 	- (5:45) Kernels and Similarity 
 	- (8:25) Example

# Lecture 12.5 — Support Vector Machines | (Kernels-II) — [Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=XfyR_49hfi8&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=74
 	- (2:15) SVM with Kernels
 	- (11:05) Why not use Kernels for in other ML algos? Why only SVM? - Dr. Ng explains that Kernels do not translate as well as they do with SVM, due to computational tricks.
 	- (12:45) How to choose parameters in SVM? 

# Lecture 12.6 — Support Vector Machines | Using An SVM — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=FCUBwP-JTsA&index=75&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- (0:15) Different Kernels
 	- (4:35) How to use Gaussian Kernel 
 		- Must do feature scalling before using Gaussian Kernels
 	- (8:35) Mercer's Theorem
 	- (12:55) Multiclass Classification
 	- (14:25) Logistic Regression vs SVM

 		- 1. If # of features (n) is higher than # of training examples (m), 
 		then use Logistic Regression or SVM without Kernel (Linear Kernel)

 		- 2. If n is small, m is intermediate,
 		then use SVM with Gaussian Kernel 

 		- 3. If n is small, m is large,
 		then create more features and use Logistic Regression or SVM without Kernel (Linear Kernel)

 		- Neural Network is likely to work well for most of these settings, but may be slower to train

##################################
# Neural Networks Representation #
##################################

# Lecture 8.1 — Neural Networks Representation | Non Linear Hypotheses — [Andrew Ng] 
 = https://www.youtube.com/watch?v=1ZhtwInuOD0&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=43
 	
 	- (0:42) Non-linear Classification 
 		- Too many features 
 			- Possibility of Overfitting 
 			- Computationally Expensive 
 	
 	- (4:52) Why Computer Vision is hard

# Lecture 8.2 — Neural Networks Representation | Neurons And The Brain — [Andrew Ng]
 = https://www.youtube.com/watch?v=m3U1_Zv4_Ik&index=44&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN

# Lecture 8.3 — Neural Networks Representation | Model Representation-I — [ Andrew Ng ]
 = https://www.youtube.com/watch?v=EVeqrPGfuCY&index=45&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- (0:15) Neurons in Brain 
 	- (2:42) Neuron model: Logistic Unit
 	- (5:32) Neural Networks (Mathematic Representation)

# Lecture 8.4 — Neural Networks Representation | Model Representation-II — [Andrew Ng]
 = https://www.youtube.com/watch?v=iPNN805konI&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=46
 	- (0:27) Forward Propagation: Vectorized Implementation 
 		- The superscript (2) means they are related with the layer 2 - which is the hidden layer 

 	- (5:58) Neural Network learning its own features 
 		- Similarity with Logistic Regression

 	- (9:47) Neural Network Architechture

# Lecture 8.5 — Neural Networks Representation | Examples And Intuitions-I — [ Andrew Ng]
 = https://www.youtube.com/watch?v=0a19YIQgRL4&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=47
 	- (0:17) Non-linear classification example: XOR/XNOR
 	- (2:12) Simple AND Example
 	- (6:17) Simple OR Example 

# Lecture 8.6 — Neural Networks Representation | Examples And Intuitions-II — [ Andrew Ng]
 = https://www.youtube.com/watch?v=0i9OhkbfNwE&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=48
 	- (0:20) Negation (~) Example
 	- (2:32) x1 XNOR x2
 		- Putting together AND, OR, (NOT x1) AND (NOT x2)
 	- (6:00) Neural Network Intuition 
 		- Interesting Example: Handwritten Digit Classification (Video)
 			- Video starts at (8:08)

# Lecture 8.7 — Neural Networks Representation | MultiClass Classification — [Andrew Ng]
 = https://www.youtube.com/watch?v=gAKQOZ5zIWg&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=49
 	- (0:28) Multiclass Classification Example


############################
# Neural Networks Learning #
############################

# Lecture 9.1 — Neural Networks Learning | Cost Function — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=0twSSFZN9Mc&index=50&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	
 	- (0:22) Neural Network Classification
 		- Binary Classification
 		- Multi-class Classification

 	- (3:03) Cost Function

# Lecture 9.2 — Neural Networks Learning | Backpropagation Algorithm — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=x_Eamf8MHwU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=51
 	- (0:12) Gradient Computation 
 	- (1:02) Gradient Computation: For One Training Sample 
 		- Forward Propagation
 	- (2:31) Gradient Computation: Backpropagation
 		- DELTA error 
 	- (7:58) Backpropagation Algorithm

# Lecture 9.3 — Neural Networks Learning | Backpropagation Intuition — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=mOmkv5SI9hU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=52
 	- (1:35) Forward Propagation
 	- (5:10) What Backpropagation is doing? 
 		- Backpropagation Intuition

# Lecture 9.4 — Neural Networks Learning | Implementation Note Unrolling Parameters — [ Andrew Ng]
 = https://www.youtube.com/watch?v=dlEoLfA4MSQ&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=53
 	- (0:20) Advanced Optimization

# Lecture 9.5 — Neural Networks Learning | Gradient Checking — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=P6EtCVrvYPU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=54
 	- (0:00) [Issue] In the implementation of Backpropagation, we can have some bug. Still the Cost Function would decrease. As a result, it might be hard to detect the bug in the first place. 
 		= Solution: Gradient Checking
 	- (1:52) Numerical Estimation of Gradient 
 		- Two-sided error calculation
 		- One-sided error calculation
 	- (5:00) Parameter Vector
 	- (8:58) Implementation Note 
 		- Turn off the gradient checking when it is close to backpropagation. 
 			- Because gradient checking takes a long time to compute partial derivatives, whereas backpropagation takes much less time

# Why Gradient Checking is slow for Backpropagation?
 = https://stackoverflow.com/questions/52779783/why-is-gradient-checking-slow-for-back-propagation


# Lecture 9.6 — Neural Networks Learning | Random Initialization — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=OF8ocg5mgx0&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=55
 	- (0:12) Random Initialization (CANNOT have THETA = 0)
 	- (1:00) Setting the initial value of THETA to zero does NOT work for Neural Networks
 		- Because the activation values (a) are going to be same for THETA = 0
 		- Similarly, the delta values (DELTA) are also going to be the same for THETA = 0
 		= Therefore, "After each update, parameters corresponding to inputs going into each of two hidden units are identical"
 	- (4:40) Random Initialization: Symmetry Breaking 

# Lecture 9.7 — Neural Networks Learning | Putting It Together — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=cObOAIImeVQ&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=56
 	- (0:20) Training a Neural Network
 		- # of input units: Dimension of features 
 		- # of output units: # of classes
 		- # of hidden layers: default 1, if > 1 hidden layer, use same # of hidden units in each layer
 			- Usually the more hidden units the better
 	- (4:07) Training a Neural Network
 		- 1. Random Initialization
 		- 2. Implement Forward Propagation to get h_THETA(x^(i)) for any x^(i)
 		- 3. Implement code to compute Cost Function J(THETA)
 		- 4. Implement Backpropagation to compute partial derivatives of J(THETA)
 		- 5. Use Gradient Checking. 
 			- Then disable Gradient Checking
 		- 6.  Use Gradient Descent or Advanced Optimization method to minimize J(THETA)
 	- Here, Backpropagation finds the local minima, because J(THETA) is non-convex.
 		- Despite this issue, Backpropagation seems to perform quite well. 
 	- Backpropagation is able to fit very complex, non-linear functions. 

# Lecture 9.8 — Neural Networks Learning | Autonomous Driving Example — [Andrew Ng]
 = https://www.youtube.com/watch?v=ppFyPUx9RIU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=57

# Backpropagation explained in 5 levels of difficulty
 = https://medium.com/coinmonks/backpropagation-concept-explained-in-5-levels-of-difficulty-8b220a939db5


# Lecture 10.1 — Advice For Applying Machine Learning | Deciding What To Try Next — [ Andrew Ng]
 = https://www.youtube.com/watch?v=sZSKGNbrwus&index=58&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- (1:10) Debugging a learning algorithm
 		- Get more training examples
 			- It does NOT help all the time
 		- Try smaller sets of features
 		- Try getting additional features
 		- Try adding polynomial features
 		- Try increasing/ decreasing LAMBDA
 	- (4:42) Machine Learning Diagnostic

# Lecture 10.2 — Advice For Applying Machine Learning | Evaluating A Hypothesis — [ Andrew Ng]
 = https://www.youtube.com/watch?v=BpgnnS7mKKU&index=59&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- (1:13) How to evaluate your hypothesis?
 		- Split the whole dataset into Training Set (70%) and Testing Set (30%)
 		- Even better: Randomly shuffle the dataset and then split into 70/30
 	- (3:43) Training/testing procedure for Linear regression
 	- (5:13) Training/testing procedure for Logistic regression
 		- 0/1 Misclassification error

# Lecture 10.3 — Advice For Applying Machine Learning | Model Selection And Train Validation Test Sets
 = https://www.youtube.com/watch?v=uoTBdCODGvk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=60
 	- (1:43) Model selection
 		- degree of polynomial 
 		- Choose the model with lowest J_test(THETA)
 		- However, this might perform bad for new data (an optimistic estimate of generalization error)
 	- (6:37) Solving generalization error problem
 		- Split into 3 sets
 			- Training (60%)
 			- Cross validation (CV) (20%)
 			- Testing (20%)
 		- Compute Train/validation/test error 
 		- Choose the model with lowest J_validation(THETA)
 		- Test the model with Test data 

# Lecture 10.4 — Advice For Applying Machine Learning | Diagnosing Bias Vs Variance — [Andrew Ng]
 = https://www.youtube.com/watch?v=fDQkUN9yw44&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=61
 	- (0:52) Bias/Variance 
 	- (4:32) Diagnosing Bias/Variance
 		- Explain Bias and Variance by "Error vs Degree of Polynomial" plot

# Lecture 10.5 — Advice For Applying Machine Learning | Regularization And Bias Variance — [Andrew Ng]
 = https://www.youtube.com/watch?v=4MKN-JkNGXY&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=62
 	- (0:22) Linear regression with Regularization 
 	- (2:30) Choosing the Regularization Parameter LAMBDA
 	- (6:57) Bias/Variance as a function of the Regularization Parameter LAMBDA


# Lecture 10.6 — Advice For Applying Machine Learning | Learning Curves — [Andrew Ng]
 = https://www.youtube.com/watch?v=ISBGFY-gBug&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=63
 	- Learning Curves 
 		- High Bias - More training data will NOT help
 		- High Variance - More training data might help
 	- Error Gap

# Lecture 10.7 — Advice For Applying Machine Learning | Deciding What To Do Next (Revisited)
 = https://www.youtube.com/watch?v=yoYA1MFpYRg&index=64&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- Debugging a learning algorithm

 		- Get more training examples
			= fixes high variance	
 		- Try smaller sets of features
 			= fixes high variance 
 		- Try getting additional features
 			= fixes high bias 
 		- Try adding polynomial features
 			= fixes high bias
 		- Try decreasing LAMBDA
 			= fixes high bias
 		- Try increasing LAMBDA
 			= fixes high variance 

 	- (3:13) Neural Networks and Overfitting

