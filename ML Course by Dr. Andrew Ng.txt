##################################
# ML Course by Dr. Andrew Ng.txt #
##################################

################
# Introduction #
################

# Lecture 1.1 — Introduction What Is Machine Learning — [ Machine Learning | Andrew Ng ]
 = https://www.youtube.com/watch?v=PPLop4L2eGk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=1
 	- (0:00) What is Machine Learning?

# Lecture 1.2 — Introduction Supervised Learning — [ Machine Learning | Andrew Ng ]
 = https://www.youtube.com/watch?v=bQI5uDxrFfA&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=2
 	- Housing Price Example
 		- Supervised Learning 
 		- Regression: Predict continuous valued output (price)
 	- Breast Cancer Example
 		- Classification 
# Lecture 1.3 — Introduction Unsupervised Learning — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=jAA2g9ItoAc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=3



#####################
# Linear Regression #
#####################

# Lecture 2.1 — Linear Regression With One Variable | Model Representation — Andrew Ng
 = https://www.youtube.com/watch?v=kHwlB_j7Hkc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=4 

# Lecture 2.2 — Linear Regression With One Variable | CostFunction — Andrew Ng
 = https://www.youtube.com/watch?v=yuH4iRcggMw&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=5

# Lecture 2.3 — Linear Regression With One Variable | Cost Function Intuition #1 | Andrew Ng
 = https://www.youtube.com/watch?v=yR2ipCoFvNo&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=6

# Lecture 2.4 — Linear Regression With One Variable | Cost Function Intuition #2 | Andrew Ng
 = https://www.youtube.com/watch?v=0kns1gXLYg4&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=7

# Lecture 2.5 — Linear Regression With One Variable | Gradient Descent — [ Andrew Ng]
 = https://www.youtube.com/watch?v=F6GSRDoB-Cg&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=8

# Lecture 2.6 — Linear Regression With One Variable | Gradient Descent Intuition — [ Andrew Ng]
 = https://www.youtube.com/watch?v=YovTqTY-PYY&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=9
 	- Gradient Descent can converge to a local minimum, even with the learning rate ALPHA fixed
 		- Since the derivative of J(THETA) becomes small as we approach local minima, the term ALPHA * d/d(THETA) [J(THETA)] also becomes small. 
 		- Therefore, we do not need to decrease ALPHA over time

# Lecture 2.7 — Linear Regression With One Variable | Gradient Descent For Linear Regression
 = https://www.youtube.com/watch?v=GtSf2T6Co80&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=10

=>	- (0:58) Derivation of d/d(THETA) [J(THETA)]
	- (7:47) Batch Gradient Descent


## Why does the Gradient Descent in Linear Regression always converge to Global Minima?
 = https://wingshore.wordpress.com/2014/11/20/gradient-descent-for-linear-regression/

#########################
# Linear Algebra Review #
#########################

# Lecture 2.8 — What's Next — [ Machine Learning | Andrew Ng | Stanford University] 
 = https://www.youtube.com/watch?v=OS7KXu0447I&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=11

# Lecture 3.1 — Linear Algebra Review | Matrices And Vectors — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=Dft1cqjwlXE&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=12

# Lecture 3.2 — Linear Algebra Review | Addition And Scalar Multiplication — [Andrew Ng]
 = https://www.youtube.com/watch?v=4WP6jVGIn7M&index=13&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN

# Lecture 3.3 — Linear Algebra Review | Matrix Vector Multiplication — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=gPegoVYp64w&index=14&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN

# Lecture 3.4 — Linear Algebra Review | Matrix-Matrix Multiplication — [ Andrew Ng ]
 = https://www.youtube.com/watch?v=_lrHXJRukMw&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=15

# Lecture 3.5 — Linear Algebra Review | Matrix Multiplication Properties — [ Andrew Ng ]
 = https://www.youtube.com/watch?v=c7GhnL2N--I&index=16&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN

# Lecture 3.6 — Linear Algebra Review | Inverse And Transpose — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=7snro4M6ukk&index=17&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN 


#############################################
# Linear Regression with Multiple Variables #
#############################################

# Lecture 4.1 — Linear Regression With Multiple Variables - (Multiple Features) — [ Andrew Ng]
 = https://www.youtube.com/watch?v=Q4GNLhRtZNc&index=18&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- Multivariate Linear Regression

# Lecture 4.2 — Linear Regression With Multiple Variables -- (Gradient Descent For Multiple Variables)
 = https://www.youtube.com/watch?v=pkJjoro-b5c&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=19

# Lecture 4.3 — Linear Regression With Multiple Variables | Gradient In Practice | Feature Scaling
 = https://www.youtube.com/watch?v=r5E2X1JdHAU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=20
 	- Feature Scaling
 	- Mean Normalization 
 	- Feature Scaling makes Gradient Descent run faster 

# Lecture 4.4 — Linear Regression With Multiple Variables | Gradient In Practice | Learning Rate
 = https://www.youtube.com/watch?v=CYlR9oYhYuY&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=21

# Lecture 4.5 — Linear Regression With Multiple Variables | Features And Polynomial Regression
 = https://www.youtube.com/watch?v=Hwj_9wMXDVo&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=22

# Lecture 4.7 — Linear Regression With Multiple Variables | Normal Equation Non Invertibility
 = https://www.youtube.com/watch?v=FZ1qPqVeMSQ&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=23

# Lecture 4.6 — Linear Regression With Multiple Variables | Normal Equation — [ Andrew Ng]
 = https://www.youtube.com/watch?v=B-Ks01zR4HY&index=24&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- (X.T*X)^(-1)*X.T*y
 	- (11:25) Gradient Descent vs Normal Equation
 		- if n (# of features) is small (<10,000) use Normal Equation
 		- For higher n, use Gradient Descent



#######################
# Logistic Regression #
#######################


# Lecture 6.1 — Logistic Regression | Classification — — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=-la3q9d7AKQ&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=32

# Lecture 6.2 — Logistic Regression | Hypothesis Representation — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=t1IT5hZfS48&index=33&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN

# Lecture 6.3 — Logistic Regression | Decision Boundary — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=F_VG4LNjZZw&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=34
 	- (4:37) Decision Boundary
 	- (9:48) Non-linear Decision Boundary
# Lecture 6.4 — Logistic Regression | Cost Function — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=HIQlmHxI6-0&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=35
 	- If we use the same cost function for Logistic Regression that we used for Linear Regression, the J(THETA) is going to be a "non-convex" function. This is because of the Sigmoid function hypothesis. 
 	- Therefore, we cannot guarrantee that we might find the global minima
 	= (4:34) Hence, we use the Logistic regression cost function

# Lecture 6.5 — Logistic Regression | Simplified Cost Function And Gradient Descent — [ Andrew Ng]
 = https://www.youtube.com/watch?v=TTdcc21Ko9A&index=36&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN

# Lecture 6.6 — Logistic Regression | Advanced Optimization — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=6vO3DVJlsK4&index=37&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN

# Lecture 6.7 — Logistic Regression | MultiClass Classification OneVsAll — [Andrew Ng]
 = https://www.youtube.com/watch?v=-EIfb6vFJzc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=38


##################
# Regularization #
##################

# Lecture 7.1 — Regularization | The Problem Of Overfitting — [ Machine Learning | Andrew Ng]




###########################
# Support Vector Machines #
###########################

# Lecture 12.1 — Support Vector Machines | Optimization Objective — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=hCOIMkcsm_g&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=70
 	- (3:00) Concept of Cost Function of SVM (in graph)
 	- (7:52) Logistic Regression vs SVM (Deriving SVM Cost Function equation)
 	- (13:52) SVM Hypothesis

# Lecture 12.2 — Support Vector Machines | Large Margin Intuition — [Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=Ccje1EzrXBU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=71
 	- (0:15) Concept of Cost Function of SVM (in graph) (again)
 	- (4:42) SVM Decision Boundary: Linearly separable case, Large margin classifier
 	- (7:12) Presence of Outliers
 
# Lecture 12.3 — Support Vector Machines | Mathematics Behind Large Margin Classification (Optional)
 = https://www.youtube.com/watch?v=QKc3Tr7U4Xc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=72

# Lecture 12.4 — Support Vector Machines | (Kernels-I) — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=mTyT-oHoivA&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=73
 	- (2:15) Kernel/Gaussian Kernal/Similarity Kernel
 	- (5:45) Kernels and Similarity 
 	- (8:25) Example

# Lecture 12.5 — Support Vector Machines | (Kernels-II) — [Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=XfyR_49hfi8&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=74
 	- (2:15) SVM with Kernels
 	- (11:05) Why not use Kernels for in other ML algos? Why only SVM? - Dr. Ng explains that Kernels do not translate as well as they do with SVM, due to computational tricks.
 	- (12:45) How to choose parameters in SVM? 

# Lecture 12.6 — Support Vector Machines | Using An SVM — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=FCUBwP-JTsA&index=75&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- (0:15) Different Kernels
 	- (4:35) How to use Gaussian Kernel 
 		- Must do feature scalling before using Gaussian Kernels
 	- (8:35) Mercer's Theorem
 	- (12:55) Multiclass Classification
 	- (14:25) Logistic Regression vs SVM

 		- 1. If # of features (n) is higher than # of training examples (m), 
 		then use Logistic Regression or SVM without Kernel (Linear Kernel)

 		- 2. If n is small, m is intermediate,
 		then use SVM with Gaussian Kernel 

 		- 3. If n is small, m is large,
 		then create more features and use Logistic Regression or SVM without Kernel (Linear Kernel)

 		- Neural Network is likely to work well for most of these settings, but may be slower to train

##################################
# Neural Networks Representation #
##################################

# Lecture 8.1 — Neural Networks Representation | Non Linear Hypotheses — [Andrew Ng] 
 = https://www.youtube.com/watch?v=1ZhtwInuOD0&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=43
 	
 	- (0:42) Non-linear Classification 
 		- Too many features 
 			- Possibility of Overfitting 
 			- Computationally Expensive 
 	
 	- (4:52) Why Computer Vision is hard

# Lecture 8.2 — Neural Networks Representation | Neurons And The Brain — [Andrew Ng]
 = https://www.youtube.com/watch?v=m3U1_Zv4_Ik&index=44&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN

# Lecture 8.3 — Neural Networks Representation | Model Representation-I — [ Andrew Ng ]
 = https://www.youtube.com/watch?v=EVeqrPGfuCY&index=45&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- (0:15) Neurons in Brain 
 	- (2:42) Neuron model: Logistic Unit
 	- (5:32) Neural Networks (Mathematic Representation)

# Lecture 8.4 — Neural Networks Representation | Model Representation-II — [Andrew Ng]
 = https://www.youtube.com/watch?v=iPNN805konI&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=46
 	- (0:27) Forward Propagation: Vectorized Implementation 
 		- The superscript (2) means they are related with the layer 2 - which is the hidden layer 

 	- (5:58) Neural Network learning its own features 
 		- Similarity with Logistic Regression

 	- (9:47) Neural Network Architechture

# Lecture 8.5 — Neural Networks Representation | Examples And Intuitions-I — [ Andrew Ng]
 = https://www.youtube.com/watch?v=0a19YIQgRL4&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=47
 	- (0:17) Non-linear classification example: XOR/XNOR
 	- (2:12) Simple AND Example
 	- (6:17) Simple OR Example 

# Lecture 8.6 — Neural Networks Representation | Examples And Intuitions-II — [ Andrew Ng]
 = https://www.youtube.com/watch?v=0i9OhkbfNwE&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=48
 	- (0:20) Negation (~) Example
 	- (2:32) x1 XNOR x2
 		- Putting together AND, OR, (NOT x1) AND (NOT x2)
 	- (6:00) Neural Network Intuition 
 		- Interesting Example: Handwritten Digit Classification (Video)
 			- Video starts at (8:08)

# Lecture 8.7 — Neural Networks Representation | MultiClass Classification — [Andrew Ng]
 = https://www.youtube.com/watch?v=gAKQOZ5zIWg&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=49
 	- (0:28) Multiclass Classification Example


############################
# Neural Networks Learning #
############################

# Lecture 9.1 — Neural Networks Learning | Cost Function — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=0twSSFZN9Mc&index=50&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	
 	- (0:22) Neural Network Classification
 		- Binary Classification
 		- Multi-class Classification

 	- (3:03) Cost Function

# Lecture 9.2 — Neural Networks Learning | Backpropagation Algorithm — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=x_Eamf8MHwU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=51
 	- (0:12) Gradient Computation 
 	- (1:02) Gradient Computation: For One Training Sample 
 		- Forward Propagation
 	- (2:31) Gradient Computation: Backpropagation
 		- DELTA error 
 	- (7:58) Backpropagation Algorithm

# Lecture 9.3 — Neural Networks Learning | Backpropagation Intuition — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=mOmkv5SI9hU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=52
 	- (1:35) Forward Propagation
 	- (5:10) What Backpropagation is doing? 
 		- Backpropagation Intuition

# Lecture 9.4 — Neural Networks Learning | Implementation Note Unrolling Parameters — [ Andrew Ng]
 = https://www.youtube.com/watch?v=dlEoLfA4MSQ&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=53
 	- (0:20) Advanced Optimization

# Lecture 9.5 — Neural Networks Learning | Gradient Checking — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=P6EtCVrvYPU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=54
 	- (0:00) [Issue] In the implementation of Backpropagation, we can have some bug. Still the Cost Function would decrease. As a result, it might be hard to detect the bug in the first place. 
 		= Solution: Gradient Checking
 	- (1:52) Numerical Estimation of Gradient 
 		- Two-sided error calculation
 		- One-sided error calculation
 	- (5:00) Parameter Vector
 	- (8:58) Implementation Note 
 		- Turn off the gradient checking when it is close to backpropagation. 
 			- Because gradient checking takes a long time to compute partial derivatives, whereas backpropagation takes much less time

# Why Gradient Checking is slow for Backpropagation?
 = https://stackoverflow.com/questions/52779783/why-is-gradient-checking-slow-for-back-propagation


# Lecture 9.6 — Neural Networks Learning | Random Initialization — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=OF8ocg5mgx0&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=55
 	- (0:12) Random Initialization (CANNOT have THETA = 0)
 	- (1:00) Setting the initial value of THETA to zero does NOT work for Neural Networks
 		- Because the activation values (a) are going to be same for THETA = 0
 		- Similarly, the delta values (DELTA) are also going to be the same for THETA = 0
 		= Therefore, "After each update, parameters corresponding to inputs going into each of two hidden units are identical"
 	- (4:40) Random Initialization: Symmetry Breaking 

# Lecture 9.7 — Neural Networks Learning | Putting It Together — [ Machine Learning | Andrew Ng]
 = https://www.youtube.com/watch?v=cObOAIImeVQ&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=56
 	- (0:20) Training a Neural Network
 		- # of input units: Dimension of features 
 		- # of output units: # of classes
 		- # of hidden layers: default 1, if > 1 hidden layer, use same # of hidden units in each layer
 			- Usually the more hidden units the better
 	- (4:07) Training a Neural Network
 		- 1. Random Initialization
 		- 2. Implement Forward Propagation to get h_THETA(x^(i)) for any x^(i)
 		- 3. Implement code to compute Cost Function J(THETA)
 		- 4. Implement Backpropagation to compute partial derivatives of J(THETA)
 		- 5. Use Gradient Checking. 
 			- Then disable Gradient Checking
 		- 6.  Use Gradient Descent or Advanced Optimization method to minimize J(THETA)
 	- Here, Backpropagation finds the local minima, because J(THETA) is non-convex.
 		- Despite this issue, Backpropagation seems to perform quite well. 
 	- Backpropagation is able to fit very complex, non-linear functions. 

# Lecture 9.8 — Neural Networks Learning | Autonomous Driving Example — [Andrew Ng]
 = https://www.youtube.com/watch?v=ppFyPUx9RIU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=57

# Backpropagation explained in 5 levels of difficulty
 = https://medium.com/coinmonks/backpropagation-concept-explained-in-5-levels-of-difficulty-8b220a939db5


# Lecture 10.1 — Advice For Applying Machine Learning | Deciding What To Try Next — [ Andrew Ng]
 = https://www.youtube.com/watch?v=sZSKGNbrwus&index=58&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- (1:10) Debugging a learning algorithm
 		- Get more training examples
 			- It does NOT help all the time
 		- Try smaller sets of features
 		- Try getting additional features
 		- Try adding polynomial features
 		- Try increasing/ decreasing LAMBDA
 	- (4:42) Machine Learning Diagnostic

# Lecture 10.2 — Advice For Applying Machine Learning | Evaluating A Hypothesis — [ Andrew Ng]
 = https://www.youtube.com/watch?v=BpgnnS7mKKU&index=59&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- (1:13) How to evaluate your hypothesis?
 		- Split the whole dataset into Training Set (70%) and Testing Set (30%)
 		- Even better: Randomly shuffle the dataset and then split into 70/30
 	- (3:43) Training/testing procedure for Linear regression
 	- (5:13) Training/testing procedure for Logistic regression
 		- 0/1 Misclassification error

# Lecture 10.3 — Advice For Applying Machine Learning | Model Selection And Train Validation Test Sets
 = https://www.youtube.com/watch?v=uoTBdCODGvk&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=60
 	- (1:43) Model selection
 		- degree of polynomial 
 		- Choose the model with lowest J_test(THETA)
 		- However, this might perform bad for new data (an optimistic estimate of generalization error)
 	- (6:37) Solving generalization error problem
 		- Split into 3 sets
 			- Training (60%)
 			- Cross validation (CV) (20%)
 			- Testing (20%)
 		- Compute Train/validation/test error 
 		- Choose the model with lowest J_validation(THETA)
 		- Test the model with Test data 

# Lecture 10.4 — Advice For Applying Machine Learning | Diagnosing Bias Vs Variance — [Andrew Ng]
 = https://www.youtube.com/watch?v=fDQkUN9yw44&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=61
 	- (0:52) Bias/Variance 
 	- (4:32) Diagnosing Bias/Variance
 		- Explain Bias and Variance by "Error vs Degree of Polynomial" plot

# Lecture 10.5 — Advice For Applying Machine Learning | Regularization And Bias Variance — [Andrew Ng]
 = https://www.youtube.com/watch?v=4MKN-JkNGXY&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=62
 	- (0:22) Linear regression with Regularization 
 	- (2:30) Choosing the Regularization Parameter LAMBDA
 	- (6:57) Bias/Variance as a function of the Regularization Parameter LAMBDA


# Lecture 10.6 — Advice For Applying Machine Learning | Learning Curves — [Andrew Ng]
 = https://www.youtube.com/watch?v=ISBGFY-gBug&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=63
 	- Learning Curves 
 		- High Bias - More training data will NOT help
 		- High Variance - More training data might help
 	- Error Gap

# Lecture 10.7 — Advice For Applying Machine Learning | Deciding What To Do Next (Revisited)
 = https://www.youtube.com/watch?v=yoYA1MFpYRg&index=64&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN
 	- Debugging a learning algorithm

 		- Get more training examples
			= fixes high variance	
 		- Try smaller sets of features
 			= fixes high variance 
 		- Try getting additional features
 			= fixes high bias 
 		- Try adding polynomial features
 			= fixes high bias
 		- Try decreasing LAMBDA
 			= fixes high bias
 		- Try increasing LAMBDA
 			= fixes high variance 

 	- (3:13) Neural Networks and Overfitting

